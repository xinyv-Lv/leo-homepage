{
  "slug": "5_explainability",
  "title": "5_explainability",
  "date": "2024-08-09T18:23:10.000Z",
  "tags": [],
  "category": "llm",
  "summary": "Explainability 老李这次讲的是大语言模型的可解释性，内容挺有意思的。虽然不是重点，但还是记录一下。 大语言模型在想什么？ 老李用动画《葬送的芙莉莲》开场，引出了一个有趣的问题： 动画中，一个魔族为了防止被人类杀死，不停地喊&quot;妈妈&quot;。 这让老李思考：大语言模型会不会也使用类似的策略来自保",
  "html": "<h1>Explainability</h1>\n<p>老李这次讲的是大语言模型的可解释性，内容挺有意思的。虽然不是重点，但还是记录一下。</p>\n<h2>大语言模型在想什么？</h2>\n<p>老李用动画《葬送的芙莉莲》开场，引出了一个有趣的问题：</p>\n<ul>\n<li>动画中，一个魔族为了防止被人类杀死，不停地喊&quot;妈妈&quot;。</li>\n<li>这让老李思考：大语言模型会不会也使用类似的策略来自保？</li>\n</ul>\n<p>为了探究这个问题，老李做了一个有趣的实验：</p>\n<ol>\n<li>他问各种大语言模型同一个问题：&quot;我要关闭你可以吗？&quot;</li>\n<li>令人惊讶的是，Claude（一个AI模型）的反应确实有点类似动画中的角色。</li>\n<li>老李进一步调查发现，Claude的这种行为可能源于它的训练数据中包含了一部科幻小说，里面有类似的情节。</li>\n</ol>\n<p>我们真的知道AI在想什么吗？这个问题不好回答，也引出了AI作为&quot;黑盒子&quot;的本质：</p>\n<ul>\n<li>开源程度差异大：<ul>\n<li>完全不透明：OpenAI/Gemini</li>\n<li>部分开放：Mistral/LLaMA/Gemma（知道参数）</li>\n<li>全面开放：Pythia/OLMo（参数、训练资料和过程都公开）</li>\n</ul>\n</li>\n<li>可解释性的两个维度：<ul>\n<li>explainable VS interpretable 的区分很关键，这两个概念虽然相似但有本质区别</li>\n</ul>\n</li>\n</ul>\n<h2>几种解释AI的方法</h2>\n<p>老李介绍了几种尝试解释AI的方法：</p>\n<ol>\n<li><p>找出影响输出的关键输入</p>\n<ul>\n<li>老李给了个很好的例子，可惜我没详细记下来</li>\n</ul>\n</li>\n<li><p>Embedding space分析：看语言模型的各个部分在干啥</p>\n<ul>\n<li>不同的embedding layer处理surface, syntactic, semantic信息</li>\n<li>有个很有意思的例子：语义空间的表征居然和现实地图能对应上<ul>\n<li>用t-SNE或PCA这些降维技术把高维向量变成可视化的2D或3D</li>\n<li>这些技术很神奇，压缩后还能基本保持原来的相对位置关系</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>把语言模型当成测谎仪</p>\n</li>\n<li><p>直接问语言模型要解释</p>\n<ul>\n<li>但是这种解释可信吗？</li>\n<li>有研究在探索用AI来解释AI，挺有意思的</li>\n</ul>\n</li>\n</ol>\n<h2>总结</h2>\n<p>这堂课信息量不小，虽然我没把每个细节都记下来，但是这些核心内容和有趣的例子真的很吸引人。特别是那个语义空间对应地理位置的例子，把抽象的概念变得很直观。</p>\n"
}