# Explainability

老李这次讲的是大语言模型的可解释性，内容挺有意思的。虽然不是重点，但还是记录一下。

## 大语言模型在想什么？


老李用动画《葬送的芙莉莲》开场，引出了一个有趣的问题：

- 动画中，一个魔族为了防止被人类杀死，不停地喊"妈妈"。
- 这让老李思考：大语言模型会不会也使用类似的策略来自保？

为了探究这个问题，老李做了一个有趣的实验：

1. 他问各种大语言模型同一个问题："我要关闭你可以吗？"
2. 令人惊讶的是，Claude（一个AI模型）的反应确实有点类似动画中的角色。
3. 老李进一步调查发现，Claude的这种行为可能源于它的训练数据中包含了一部科幻小说，里面有类似的情节。

我们真的知道AI在想什么吗？这个问题不好回答，也引出了AI作为"黑盒子"的本质：
- 开源程度差异大：
  - 完全不透明：OpenAI/Gemini
  - 部分开放：Mistral/LLaMA/Gemma（知道参数）
  - 全面开放：Pythia/OLMo（参数、训练资料和过程都公开）
- 可解释性的两个维度：
  - explainable VS interpretable 的区分很关键，这两个概念虽然相似但有本质区别


## 几种解释AI的方法

老李介绍了几种尝试解释AI的方法：

1. 找出影响输出的关键输入
   - 老李给了个很好的例子，可惜我没详细记下来

2. Embedding space分析：看语言模型的各个部分在干啥
   - 不同的embedding layer处理surface, syntactic, semantic信息
   - 有个很有意思的例子：语义空间的表征居然和现实地图能对应上
     - 用t-SNE或PCA这些降维技术把高维向量变成可视化的2D或3D
     - 这些技术很神奇，压缩后还能基本保持原来的相对位置关系

3. 把语言模型当成测谎仪

4. 直接问语言模型要解释
   - 但是这种解释可信吗？
   - 有研究在探索用AI来解释AI，挺有意思的

## 总结

这堂课信息量不小，虽然我没把每个细节都记下来，但是这些核心内容和有趣的例子真的很吸引人。特别是那个语义空间对应地理位置的例子，把抽象的概念变得很直观。
